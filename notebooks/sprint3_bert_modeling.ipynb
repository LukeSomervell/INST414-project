{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading\n",
    "\n",
    "In this step, we set up the environment by importing necessary libraries and loading the processed dataset.\n",
    "We will use only the email `body` text and its associated `label` (0 = legitimate, 1 = phishing) as inputs for the BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body  label\n",
      "0  buck up, your troubles caused by small dimensi...      1\n",
      "1  \\nupgrade your sex and pleasures with these te...      1\n",
      "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1\n",
      "3  would anyone object to removing .so from this ...      0\n",
      "4  \\nwelcomefastshippingcustomersupport\\nhttp://7...      1\n",
      "label\n",
      "1    21827\n",
      "0    17312\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_csv(\"../data/processed/CEAS_08_feature_engineered.csv\")\n",
    "\n",
    "# Keep only the email body and label\n",
    "df = df[['body', 'label']]\n",
    "\n",
    "# Quick preview\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train/Test Split\n",
    "\n",
    "We will split the data into training and testing sets.\n",
    "- 80% of the emails will be used for training the BERT model.\n",
    "- 20% of the emails will be used for final evaluation.\n",
    "Stratified sampling will be used to ensure both classes (phishing and legitimate) are represented proportionally in both sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 31311\n",
      "Number of testing samples: 7828\n"
     ]
    }
   ],
   "source": [
    "# Perform train/test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['body'].tolist(),\n",
    "    df['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(f\"Number of training samples: {len(train_texts)}\")\n",
    "print(f\"Number of testing samples: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization\n",
    "\n",
    "We will use a pre-trained BERT tokenizer to convert the email text into numerical tokens that BERT can process.\n",
    "Each email will be tokenized into:\n",
    "- Input IDs: the numerical representation of each word piece.\n",
    "- Attention Masks: a binary mask indicating which tokens should be attended to.\n",
    "\n",
    "We will use the \"bert-base-uncased\" model from Hugging Face for tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokenized input IDs: tensor([  101,  7314,  2024,  2734,  2005,  9441,  6887, 17175,  2705,  2239,\n",
      "         2263,  1012,  2000,  5589,  1999,  9441,  6887, 17175,  2705,  2239,\n",
      "         2263,  1010,  3531,  3967,  2472,  1997,  3296,  3228,  7437, 24185,\n",
      "         3363,  2386,  2011,  1041,  1011,  5653,  2012,  1041,  4140,  2100,\n",
      "         7677, 13535,  1030, 27302, 28139,  2361,  1012,  8917,  2030,  2011,\n",
      "         7026,  2012,  6390,  2620,  1011,  4029,  2581,  1011,  9683,  2692,\n",
      "         4654,  2102,  1012, 19348,  1012,  1996,  5246,  1997,  9441,  6887,\n",
      "        17175,  2705,  2239,  2263,  2024,  1024, 14803,  2233,  1016,  2233,\n",
      "         1023, 28401,  2337,  2423,  2233,  1017,  2233,  2184,  9857,  2015,\n",
      "         2337,  2656,  2233,  1018,  2233,  2340,  9317,  2015,  2337,  2676,\n",
      "         2233,  2260,  2000,  2424,  2041,  2062,  2055,  2054,  1005,  1055,\n",
      "         6230,  2012,  1996, 17463,  1010,  3942,  8299,  1024,  1013,  1013,\n",
      "         7479,  1012, 27302, 28139,  2361,  1012,  8917,  1012,  2000,  6366,\n",
      "         2115,  2171,  2013,  2023,  2862,  1010,  4604,  1037,  4471,  2000,\n",
      "         1048,  3501,  2480, 13578,  2099,  2615,  1030, 27302, 28139,  2361,\n",
      "         1012,  8917,  2007,  1996,  2616,  1000,  3696,  7245,  9441,  1000,\n",
      "         1999,  1996,  2303,  1997,  1996,  4471,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "Example attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the training and testing texts\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,  # Truncate longer emails\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(\"Example tokenized input IDs:\", train_encodings['input_ids'][0])\n",
    "print(\"Example attention mask:\", train_encodings['attention_mask'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Initialization\n",
    "\n",
    "We will initialize a BERT model pre-trained on English text (\"bert-base-uncased\") for a binary classification task (phishing vs. legitimate).\n",
    "The model will output a probability for each class (0 or 1).\n",
    "We will also set up the datasets needed for training using the tokenized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification (binary classification)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class PhishingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create the training and testing datasets\n",
    "train_dataset = PhishingDataset(train_encodings, train_labels)\n",
    "test_dataset = PhishingDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training the BERT Model\n",
    "\n",
    "We will fine-tune the BERT model on the phishing email classification dataset.\n",
    "Due to version differences in libraries, we moved evaluation control into the Trainer configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,             \u001b[38;5;66;03m# where to save model checkpoints\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,                  \u001b[38;5;66;03m# number of training epochs\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,      \u001b[38;5;66;03m# batch size for training\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,       \u001b[38;5;66;03m# batch size for evaluation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,                    \u001b[38;5;66;03m# number of warmup steps\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,                   \u001b[38;5;66;03m# strength of weight decay\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,         \u001b[38;5;66;03m# evaluate at the end of each epoch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,               \u001b[38;5;66;03m# save model after each epoch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,          \u001b[38;5;66;03m# load the best model (highest metric) at the end\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m,           \u001b[38;5;66;03m# optimize for F1-score\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,               \u001b[38;5;66;03m# higher F1 is better\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,                 \u001b[38;5;66;03m# directory for logs\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,                     \u001b[38;5;66;03m# log every 10 steps\u001b[39;00m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Define the Trainer\u001b[39;00m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     25\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     26\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             # where to save model checkpoints\n",
    "    num_train_epochs=2,                  # number of training epochs\n",
    "    per_device_train_batch_size=16,      # batch size for training\n",
    "    per_device_eval_batch_size=64,       # batch size for evaluation\n",
    "    warmup_steps=500,                    # number of warmup steps\n",
    "    weight_decay=0.01,                   # strength of weight decay\n",
    "    evaluation_strategy=\"epoch\",         # evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",               # save model after each epoch\n",
    "    load_best_model_at_end=True,          # load the best model (highest metric) at the end\n",
    "    metric_for_best_model=\"f1\",           # optimize for F1-score\n",
    "    greater_is_better=True,               # higher F1 is better\n",
    "    logging_dir='./logs',                 # directory for logs\n",
    "    logging_steps=10,                     # log every 10 steps\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
